"""
GPU ì •ë³´ ìœ í‹¸ë¦¬í‹°
NVIDIA GPU ì •ë³´ ì¡°íšŒ ë° ëª¨ë‹ˆí„°ë§
"""

import subprocess
from typing import Dict, Optional
from utils.logger import logger


class GPUInfo:
    """GPU ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤"""

    def __init__(self):
        self.has_gpu = False
        self.gpu_name = "No GPU detected"
        self.use_pynvml = False
        self.use_torch = False
        self.try_pynvml()
        if not self.has_gpu:
            self.try_torch()

    def try_pynvml(self):
        """pynvmlì„ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (ê¶Œì¥)"""
        try:
            import pynvml
            pynvml.nvmlInit()
            self.has_gpu = True
            self.use_pynvml = True
            logger.info("GPU monitoring enabled via pynvml")
        except Exception as e:
            logger.debug(f"pynvml not available: {str(e)}")
            self.has_gpu = False

    def try_torch(self):
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (fallback)"""
        try:
            import torch
            if torch.cuda.is_available():
                self.has_gpu = True
                self.use_torch = True
                logger.info("GPU monitoring enabled via PyTorch")
            else:
                logger.info("No CUDA GPU detected")
                self.has_gpu = False
        except Exception as e:
            logger.debug(f"PyTorch GPU check failed: {str(e)}")
            self.has_gpu = False

    def get_gpu_info(self) -> Dict[str, str]:
        """
        GPU ì •ë³´ ì¡°íšŒ

        Returns:
            Dict: GPU ì •ë³´
                - name: GPU ì´ë¦„
                - memory_used: ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (GB)
                - memory_total: ì „ì²´ ë©”ëª¨ë¦¬ (GB)
                - utilization: GPU ì‚¬ìš©ë¥  (%)
                - status: ìƒíƒœ ë©”ì‹œì§€
        """
        if not self.has_gpu:
            return {
                "name": "GPU not detected",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": "No GPU available"
            }

        # pynvml ë°©ì‹ (ë” ìì„¸í•œ ì •ë³´)
        if self.use_pynvml:
            return self._get_gpu_info_pynvml()

        # PyTorch ë°©ì‹ (fallback)
        if self.use_torch:
            return self._get_gpu_info_torch()

        return {
            "name": "GPU not detected",
            "memory_used": "-",
            "memory_total": "-",
            "utilization": "-",
            "status": "No GPU available"
        }

    def _get_gpu_info_pynvml(self) -> Dict[str, str]:
        """pynvmlì„ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import pynvml

            # GPU 0 (ì²« ë²ˆì§¸ GPU) ì •ë³´ ì¡°íšŒ
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count == 0:
                return {
                    "name": "No GPU available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "No NVIDIA GPU detected"
                }

            handle = pynvml.nvmlDeviceGetHandleByIndex(0)

            # GPU ì´ë¦„
            gpu_name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(gpu_name, bytes):
                gpu_name = gpu_name.decode('utf-8')

            # ë©”ëª¨ë¦¬ ì •ë³´
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            memory_used_mb = mem_info.used / (1024 * 1024)
            memory_total_mb = mem_info.total / (1024 * 1024)
            memory_used_gb = memory_used_mb / 1024
            memory_total_gb = memory_total_mb / 1024

            # GPU ì‚¬ìš©ë¥ 
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = utilization.gpu
            except pynvml.NVMLError:
                gpu_utilization = 0

            return {
                "name": gpu_name,
                "memory_used": f"{memory_used_gb:.1f}",
                "memory_total": f"{memory_total_gb:.1f}",
                "utilization": str(int(gpu_utilization)),
                "status": "OK"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via pynvml: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}"
            }

    def _get_gpu_info_torch(self) -> Dict[str, str]:
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import torch

            if not torch.cuda.is_available():
                return {
                    "name": "CUDA not available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "CUDA disabled"
                }

            # GPU ì´ë¦„
            gpu_name = torch.cuda.get_device_name(0)

            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)  # GB
            memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)  # GB

            return {
                "name": gpu_name,
                "memory_used": f"{memory_allocated:.1f}",
                "memory_total": f"{memory_reserved:.1f}",
                "utilization": "-",  # PyTorchë¡œëŠ” ì‚¬ìš©ë¥ ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
                "status": "OK (PyTorch)"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via PyTorch: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}"
            }

    def shutdown(self):
        """GPU ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        try:
            import pynvml
            pynvml.nvmlShutdown()
        except Exception:
            pass


# ì „ì—­ GPU ì •ë³´ ê°ì²´
_gpu_info = None


def get_gpu_info() -> Dict[str, str]:
    """
    GPU ì •ë³´ ì¡°íšŒ (ê¸€ë¡œë²Œ í•¨ìˆ˜)

    Returns:
        Dict: GPU ì •ë³´
    """
    global _gpu_info
    if _gpu_info is None:
        _gpu_info = GPUInfo()

    return _gpu_info.get_gpu_info()


def get_gpu_display_text() -> str:
    """
    GUI í‘œì‹œìš© GPU ì •ë³´ ë¬¸ìì—´ ìƒì„±

    Returns:
        str: í¬ë§·ëœ GPU ì •ë³´ ë¬¸ìì—´
    """
    info = get_gpu_info()

    if info["status"] != "OK":
        return f"ğŸ® {info['name']}"

    memory = f"{info['memory_used']}GB / {info['memory_total']}GB"
    utilization = f"{info['utilization']}%"

    return f"ğŸ® {info['name']}  |  ì‚¬ìš©ë¥ : {utilization}  |  ë©”ëª¨ë¦¬: {memory}"
