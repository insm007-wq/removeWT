"""
GPU ì •ë³´ ìœ í‹¸ë¦¬í‹°
NVIDIA GPU ì •ë³´ ì¡°íšŒ ë° ëª¨ë‹ˆí„°ë§
"""

import subprocess
import psutil
from typing import Dict, Optional
from utils.logger import logger


class GPUInfo:
    """GPU ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤"""

    def __init__(self):
        self.has_gpu = False
        self.gpu_name = "No GPU detected"
        self.use_pynvml = False
        self.use_torch = False
        self.try_pynvml()
        if not self.has_gpu:
            self.try_torch()

    def try_pynvml(self):
        """pynvmlì„ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (ê¶Œì¥)"""
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count > 0:
                self.has_gpu = True
                self.use_pynvml = True
                logger.info(f"GPU monitoring enabled via pynvml ({device_count} GPU(s) found)")
            else:
                logger.info("pynvml initialized but no GPUs found")
                self.has_gpu = False
        except Exception as e:
            logger.debug(f"pynvml not available: {str(e)}")
            self.has_gpu = False

    def try_torch(self):
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (fallback)"""
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            device_count = torch.cuda.device_count() if cuda_available else 0

            if cuda_available and device_count > 0:
                self.has_gpu = True
                self.use_torch = True
                logger.info(f"GPU monitoring enabled via PyTorch ({device_count} GPU(s) found)")
            else:
                logger.info(f"PyTorch CUDA check - Available: {cuda_available}, Count: {device_count}")
                self.has_gpu = False
        except Exception as e:
            logger.warning(f"PyTorch GPU check failed: {str(e)}")
            self.has_gpu = False

    def get_gpu_info(self) -> Dict[str, str]:
        """
        GPU ì •ë³´ ì¡°íšŒ

        Returns:
            Dict: GPU ì •ë³´
                - name: GPU ì´ë¦„
                - memory_used: ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (GB)
                - memory_total: ì „ì²´ ë©”ëª¨ë¦¬ (GB)
                - utilization: GPU ì‚¬ìš©ë¥  (%)
                - status: ìƒíƒœ ë©”ì‹œì§€
        """
        if not self.has_gpu:
            return {
                "name": "GPU not detected",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": "No GPU available"
            }

        # pynvml ë°©ì‹ (ë” ìì„¸í•œ ì •ë³´)
        if self.use_pynvml:
            return self._get_gpu_info_pynvml()

        # PyTorch ë°©ì‹ (fallback)
        if self.use_torch:
            return self._get_gpu_info_torch()

        return {
            "name": "GPU not detected",
            "memory_used": "-",
            "memory_total": "-",
            "utilization": "-",
            "status": "No GPU available"
        }

    def _get_gpu_info_pynvml(self) -> Dict[str, str]:
        """pynvmlì„ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import pynvml

            # GPU 0 (ì²« ë²ˆì§¸ GPU) ì •ë³´ ì¡°íšŒ
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count == 0:
                return {
                    "name": "No GPU available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "No NVIDIA GPU detected"
                }

            handle = pynvml.nvmlDeviceGetHandleByIndex(0)

            # GPU ì´ë¦„
            gpu_name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(gpu_name, bytes):
                gpu_name = gpu_name.decode('utf-8')

            # ë©”ëª¨ë¦¬ ì •ë³´
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            memory_used_mb = mem_info.used / (1024 * 1024)
            memory_total_mb = mem_info.total / (1024 * 1024)
            memory_used_gb = memory_used_mb / 1024
            memory_total_gb = memory_total_mb / 1024

            # GPU ì‚¬ìš©ë¥ 
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = utilization.gpu
            except pynvml.NVMLError:
                gpu_utilization = 0

            return {
                "name": gpu_name,
                "memory_used": f"{memory_used_gb:.1f}",
                "memory_total": f"{memory_total_gb:.1f}",
                "utilization": str(int(gpu_utilization)),
                "status": "OK"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via pynvml: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}"
            }

    def _get_gpu_info_torch(self) -> Dict[str, str]:
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import torch

            if not torch.cuda.is_available():
                return {
                    "name": "CUDA not available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "CUDA disabled"
                }

            # GPU ì´ë¦„
            gpu_name = torch.cuda.get_device_name(0)

            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)  # GB
            memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)  # GB

            return {
                "name": gpu_name,
                "memory_used": f"{memory_allocated:.1f}",
                "memory_total": f"{memory_reserved:.1f}",
                "utilization": "-",  # PyTorchë¡œëŠ” ì‚¬ìš©ë¥ ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
                "status": "OK (PyTorch)"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via PyTorch: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}"
            }

    def shutdown(self):
        """GPU ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        try:
            import pynvml
            pynvml.nvmlShutdown()
        except Exception:
            pass


# ì „ì—­ GPU ì •ë³´ ê°ì²´ (ìºì‹œ)
_gpu_info = None
_last_detection_attempt = None


def get_gpu_info() -> Dict[str, str]:
    """
    GPU ì •ë³´ ì¡°íšŒ (ê¸€ë¡œë²Œ í•¨ìˆ˜)
    ë§¤ë²ˆ GPU ê°ì§€ë¥¼ ì¬ì‹œë„í•˜ì—¬ ë™ì  ë¡œë“œ ì§€ì›

    Returns:
        Dict: GPU ì •ë³´
    """
    global _gpu_info

    # GPU ì •ë³´ ê°ì²´ê°€ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ì¬ê°ì§€ ì‹œë„
    if _gpu_info is None:
        _gpu_info = GPUInfo()
    else:
        # ì´ë¯¸ GPU ê°ì§€ ì‹œë„í–ˆë‹¤ë©´, ë‹¤ì‹œ ì‹œë„í•´ë´„ (CUDA ë™ì  ë¡œë“œ ì§€ì›)
        if not _gpu_info.has_gpu:
            _gpu_info = GPUInfo()

    return _gpu_info.get_gpu_info()


def get_gpu_display_text() -> str:
    """
    GUI í‘œì‹œìš© GPU ì •ë³´ ë¬¸ìì—´ ìƒì„±
    GPUì™€ CPU ì •ë³´ í•¨ê»˜ í‘œì‹œ (PyTorch ìš°ì„ , pynvml í´ë°±)

    Returns:
        str: í¬ë§·ëœ GPU/CPU ì •ë³´ ë¬¸ìì—´
    """
    try:
        gpu_text = ""

        # ===== PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (ìš°ì„  ë°©ì‹) =====
        try:
            import torch

            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)  # GB
                memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)    # GB

                # ì‹¤ì œ total ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
                props = torch.cuda.get_device_properties(0)
                memory_total_gb = props.total_memory / (1024 ** 3)

                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° (í• ë‹¹ëœ ë©”ëª¨ë¦¬ ê¸°ì¤€)
                memory_usage_percent = (memory_allocated / memory_total_gb) * 100 if memory_total_gb > 0 else 0

                gpu_text = f"ğŸ® {gpu_name}  |  ë©”ëª¨ë¦¬: {memory_allocated:.1f}GB / {memory_total_gb:.1f}GB ({memory_usage_percent:.0f}%)"

                # GPU ì‚¬ìš©ë¥ ì€ nvidia-smië¡œ ê°€ì ¸ì˜¤ê¸° (ë³´ì¡°)
                gpu_util_str = ""
                try:
                    result = subprocess.run(
                        ["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"],
                        capture_output=True,
                        text=True,
                        timeout=1
                    )
                    if result.returncode == 0:
                        gpu_util = result.stdout.strip().split('\n')[0].strip()
                        if gpu_util and gpu_util.isdigit():
                            gpu_util_str = f"  |  GPU í™œìš©: {gpu_util}%"
                except Exception as e:
                    logger.debug(f"nvidia-smi error: {str(e)}")

                # gpu_text += gpu_util_str  # GPU í™œìš©ë¥  í‘œì‹œ (ìˆ¨ê¹€ - ë‚˜ì¤‘ì— í•„ìš”ì‹œ í™œì„±í™”)
            else:
                gpu_text = "ğŸ® CUDA not available"

        except ImportError:
            logger.debug("PyTorch not available, trying pynvml...")
            # PyTorch ì—†ìœ¼ë©´ nvidia-smië¡œ ì‹œë„
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=name,memory.used,memory.total,utilization.gpu", "--format=csv,noheader,nounits"],
                    capture_output=True,
                    text=True,
                    timeout=2
                )
                if result.returncode == 0:
                    output = result.stdout.strip().split('\n')[0]
                    parts = [p.strip() for p in output.split(',')]
                    if len(parts) >= 4:
                        gpu_name = parts[0]
                        memory_used = float(parts[1]) / 1024  # MB to GB
                        memory_total = float(parts[2]) / 1024  # MB to GB
                        gpu_util = parts[3]
                        # GPU í™œìš©ë¥  í‘œì‹œ ì œê±° (ìˆ¨ê¹€ - ë‚˜ì¤‘ì— í•„ìš”ì‹œ ì¶”ê°€)
                        gpu_text = f"ğŸ® {gpu_name}  |  ë©”ëª¨ë¦¬: {memory_used:.1f}GB / {memory_total:.1f}GB"
                    else:
                        gpu_text = "ğŸ® GPU not detected"
                else:
                    gpu_text = "ğŸ® GPU not detected"
            except Exception as e:
                logger.debug(f"nvidia-smi error: {str(e)}")
                gpu_text = "ğŸ® GPU not detected"

        # ===== CPU ì •ë³´ ì¶”ê°€ (ë¹„ë¸”ë¡œí‚¹ ìƒ˜í”Œë§) =====
        try:
            cpu_percent = psutil.cpu_percent(interval=None)  # Non-blocking (ìµœê·¼ ê°’ ì‚¬ìš©)
            gpu_text += f"  |  CPU: {cpu_percent}%"
        except Exception as e:
            logger.debug(f"CPU info error: {str(e)}")

        return gpu_text

    except Exception as e:
        logger.warning(f"Failed to get display text: {str(e)}")
        return "ğŸ® System info unavailable"
