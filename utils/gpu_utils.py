"""
GPU ì •ë³´ ìœ í‹¸ë¦¬í‹°
NVIDIA GPU ì •ë³´ ì¡°íšŒ ë° ëª¨ë‹ˆí„°ë§
"""

import subprocess
import psutil
from typing import Dict, Optional
from utils.logger import logger


class GPUInfo:
    """GPU ì •ë³´ ì¡°íšŒ í´ë˜ìŠ¤ (NVIDIA CUDA ë° AMD ROCm ì§€ì›)"""

    def __init__(self):
        self.has_gpu = False
        self.gpu_name = "No GPU detected"
        self.gpu_type = None  # "cuda", "rocm", or None
        self.use_pynvml = False
        self.use_torch = False
        self.use_rocm = False

        # GPU ê°ì§€ ìˆœì„œ: pynvml (NVIDIA) -> ROCm (AMD) -> PyTorch (Fallback)
        self.try_pynvml()
        if not self.has_gpu:
            self.try_rocm()
        if not self.has_gpu:
            self.try_torch()

    def try_pynvml(self):
        """pynvmlì„ ì‚¬ìš©í•œ NVIDIA GPU ì •ë³´ ì¡°íšŒ (ê¶Œì¥)"""
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count > 0:
                self.has_gpu = True
                self.gpu_type = "cuda"
                self.use_pynvml = True
                logger.info(f"GPU monitoring enabled via pynvml ({device_count} NVIDIA GPU(s) found)")
            else:
                logger.debug("pynvml initialized but no GPUs found")
                self.has_gpu = False
        except Exception as e:
            logger.debug(f"pynvml not available: {str(e)}")
            self.has_gpu = False

    def try_rocm(self):
        """ROCmì„ ì‚¬ìš©í•œ AMD GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import torch
            if torch.cuda.is_available() and "HIP" in torch.version.cuda:
                # ROCmì€ torch.cuda.is_available()ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ HIP ë°±ì—”ë“œ ì‚¬ìš©
                device_count = torch.cuda.device_count()
                if device_count > 0:
                    self.has_gpu = True
                    self.gpu_type = "rocm"
                    self.use_rocm = True
                    logger.info(f"GPU monitoring enabled via ROCm ({device_count} AMD GPU(s) found)")
                    return
        except Exception as e:
            logger.debug(f"ROCm check via HIP failed: {str(e)}")

        # ëŒ€ì²´ ë°©ë²•: rocm-smi ëª…ë ¹ì–´ ì‚¬ìš©
        try:
            result = subprocess.run(
                ["rocm-smi"],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0 and "GPU" in result.stdout:
                self.has_gpu = True
                self.gpu_type = "rocm"
                self.use_rocm = True
                logger.info("GPU monitoring enabled via ROCm (rocm-smi detected)")
        except Exception as e:
            logger.debug(f"rocm-smi not available: {str(e)}")
            self.has_gpu = False

    def try_torch(self):
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (fallback)"""
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            device_count = torch.cuda.device_count() if cuda_available else 0

            if cuda_available and device_count > 0:
                self.has_gpu = True
                # ì´ë¯¸ ROCmìœ¼ë¡œ ê°ì§€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ CUDAë¡œ í‘œì‹œ
                if not self.gpu_type:
                    self.gpu_type = "cuda"
                self.use_torch = True
                logger.info(f"GPU monitoring enabled via PyTorch ({device_count} GPU(s) found)")
            else:
                logger.info(f"PyTorch CUDA check - Available: {cuda_available}, Count: {device_count}")
                self.has_gpu = False
        except Exception as e:
            logger.warning(f"PyTorch GPU check failed: {str(e)}")
            self.has_gpu = False

    def get_gpu_info(self) -> Dict[str, str]:
        """
        GPU ì •ë³´ ì¡°íšŒ (NVIDIA CUDA ë° AMD ROCm ì§€ì›)

        Returns:
            Dict: GPU ì •ë³´
                - name: GPU ì´ë¦„
                - memory_used: ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (GB)
                - memory_total: ì „ì²´ ë©”ëª¨ë¦¬ (GB)
                - utilization: GPU ì‚¬ìš©ë¥  (%)
                - status: ìƒíƒœ ë©”ì‹œì§€
                - type: GPU íƒ€ì… ("cuda" ë˜ëŠ” "rocm")
        """
        if not self.has_gpu:
            return {
                "name": "GPU not detected",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": "No GPU available",
                "type": None
            }

        # pynvml ë°©ì‹ - NVIDIA GPU (ë” ìì„¸í•œ ì •ë³´)
        if self.use_pynvml:
            return self._get_gpu_info_pynvml()

        # ROCm ë°©ì‹ - AMD GPU
        if self.use_rocm:
            return self._get_gpu_info_rocm()

        # PyTorch ë°©ì‹ (fallback)
        if self.use_torch:
            return self._get_gpu_info_torch()

        return {
            "name": "GPU not detected",
            "memory_used": "-",
            "memory_total": "-",
            "utilization": "-",
            "status": "No GPU available",
            "type": None
        }

    def _get_gpu_info_pynvml(self) -> Dict[str, str]:
        """pynvmlì„ ì‚¬ìš©í•œ NVIDIA GPU ì •ë³´ ì¡°íšŒ"""
        try:
            import pynvml

            # GPU 0 (ì²« ë²ˆì§¸ GPU) ì •ë³´ ì¡°íšŒ
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count == 0:
                return {
                    "name": "No GPU available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "No NVIDIA GPU detected",
                    "type": "cuda"
                }

            handle = pynvml.nvmlDeviceGetHandleByIndex(0)

            # GPU ì´ë¦„
            gpu_name = pynvml.nvmlDeviceGetName(handle)
            if isinstance(gpu_name, bytes):
                gpu_name = gpu_name.decode('utf-8')

            # ë©”ëª¨ë¦¬ ì •ë³´
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            memory_used_mb = mem_info.used / (1024 * 1024)
            memory_total_mb = mem_info.total / (1024 * 1024)
            memory_used_gb = memory_used_mb / 1024
            memory_total_gb = memory_total_mb / 1024

            # GPU ì‚¬ìš©ë¥ 
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = utilization.gpu
            except pynvml.NVMLError:
                gpu_utilization = 0

            return {
                "name": gpu_name,
                "memory_used": f"{memory_used_gb:.1f}",
                "memory_total": f"{memory_total_gb:.1f}",
                "utilization": str(int(gpu_utilization)),
                "status": "OK",
                "type": "cuda"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via pynvml: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}",
                "type": "cuda"
            }

    def _get_gpu_info_rocm(self) -> Dict[str, str]:
        """rocm-smië¥¼ ì‚¬ìš©í•œ AMD GPU ì •ë³´ ì¡°íšŒ"""
        try:
            result = subprocess.run(
                ["rocm-smi", "--showid", "--showmeminfo=vram", "--json"],
                capture_output=True,
                text=True,
                timeout=2
            )

            if result.returncode == 0:
                import json
                try:
                    data = json.loads(result.stdout)
                    if isinstance(data, list) and len(data) > 0:
                        gpu_info = data[0]
                        gpu_name = gpu_info.get("gpu_id", "AMD GPU")

                        # ë©”ëª¨ë¦¬ ì •ë³´ ì¶”ì¶œ
                        mem_info = gpu_info.get("mem_info", {})
                        if isinstance(mem_info, dict):
                            memory_used_mb = int(mem_info.get("vram", {}).get("used", 0))
                            memory_total_mb = int(mem_info.get("vram", {}).get("total", 0))
                        else:
                            memory_used_mb = 0
                            memory_total_mb = 0

                        memory_used_gb = memory_used_mb / 1024
                        memory_total_gb = memory_total_mb / 1024

                        return {
                            "name": f"AMD {gpu_name}",
                            "memory_used": f"{memory_used_gb:.1f}",
                            "memory_total": f"{memory_total_gb:.1f}",
                            "utilization": "-",
                            "status": "OK",
                            "type": "rocm"
                        }
                except Exception as e:
                    logger.debug(f"Error parsing rocm-smi JSON: {str(e)}")

            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ rocm-smi ì‚¬ìš©
            result = subprocess.run(
                ["rocm-smi"],
                capture_output=True,
                text=True,
                timeout=2
            )

            if result.returncode == 0:
                lines = result.stdout.split('\n')
                for line in lines:
                    if "GPU" in line and ":" in line:
                        gpu_name = line.split(":")[0].strip()
                        return {
                            "name": f"AMD {gpu_name}",
                            "memory_used": "-",
                            "memory_total": "-",
                            "utilization": "-",
                            "status": "OK",
                            "type": "rocm"
                        }

            return {
                "name": "AMD GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": "OK",
                "type": "rocm"
            }

        except Exception as e:
            logger.debug(f"Error getting GPU info via rocm-smi: {str(e)}")
            return {
                "name": "Error reading AMD GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}",
                "type": "rocm"
            }

    def _get_gpu_info_torch(self) -> Dict[str, str]:
        """PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (CUDA ë˜ëŠ” ROCm)"""
        try:
            import torch

            if not torch.cuda.is_available():
                return {
                    "name": "CUDA not available",
                    "memory_used": "-",
                    "memory_total": "-",
                    "utilization": "-",
                    "status": "CUDA/ROCm disabled",
                    "type": self.gpu_type or "unknown"
                }

            # GPU ì´ë¦„
            gpu_name = torch.cuda.get_device_name(0)

            # ë©”ëª¨ë¦¬ ì •ë³´
            memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)  # GB
            memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)  # GB

            return {
                "name": gpu_name,
                "memory_used": f"{memory_allocated:.1f}",
                "memory_total": f"{memory_reserved:.1f}",
                "utilization": "-",  # PyTorchë¡œëŠ” ì‚¬ìš©ë¥ ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
                "status": "OK (PyTorch)",
                "type": self.gpu_type or "cuda"
            }

        except Exception as e:
            logger.warning(f"Error getting GPU info via PyTorch: {str(e)}")
            return {
                "name": "Error reading GPU",
                "memory_used": "-",
                "memory_total": "-",
                "utilization": "-",
                "status": f"Error: {str(e)}",
                "type": self.gpu_type or "unknown"
            }

    def shutdown(self):
        """GPU ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        try:
            import pynvml
            pynvml.nvmlShutdown()
        except Exception:
            pass


# ì „ì—­ GPU ì •ë³´ ê°ì²´ (ìºì‹œ)
_gpu_info = None
_last_detection_attempt = None


def get_gpu_info() -> Dict[str, str]:
    """
    GPU ì •ë³´ ì¡°íšŒ (ê¸€ë¡œë²Œ í•¨ìˆ˜)
    ë§¤ë²ˆ GPU ê°ì§€ë¥¼ ì¬ì‹œë„í•˜ì—¬ ë™ì  ë¡œë“œ ì§€ì›

    Returns:
        Dict: GPU ì •ë³´
    """
    global _gpu_info

    # GPU ì •ë³´ ê°ì²´ê°€ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ì¬ê°ì§€ ì‹œë„
    if _gpu_info is None:
        _gpu_info = GPUInfo()
    else:
        # ì´ë¯¸ GPU ê°ì§€ ì‹œë„í–ˆë‹¤ë©´, ë‹¤ì‹œ ì‹œë„í•´ë´„ (CUDA ë™ì  ë¡œë“œ ì§€ì›)
        if not _gpu_info.has_gpu:
            _gpu_info = GPUInfo()

    return _gpu_info.get_gpu_info()


def get_gpu_display_text() -> str:
    """
    GUI í‘œì‹œìš© GPU ì •ë³´ ë¬¸ìì—´ ìƒì„±
    GPU(NVIDIA/AMD)ì™€ CPU ì •ë³´ í•¨ê»˜ í‘œì‹œ (PyTorch ìš°ì„ , pynvml/rocm-smi í´ë°±)

    Returns:
        str: í¬ë§·ëœ GPU/CPU ì •ë³´ ë¬¸ìì—´
    """
    try:
        gpu_text = ""

        # ===== PyTorchë¥¼ ì‚¬ìš©í•œ GPU ì •ë³´ ì¡°íšŒ (ìš°ì„  ë°©ì‹) =====
        try:
            import torch

            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                memory_allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)  # GB
                memory_reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)    # GB

                # ì‹¤ì œ total ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
                props = torch.cuda.get_device_properties(0)
                memory_total_gb = props.total_memory / (1024 ** 3)

                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° (í• ë‹¹ëœ ë©”ëª¨ë¦¬ ê¸°ì¤€)
                memory_usage_percent = (memory_allocated / memory_total_gb) * 100 if memory_total_gb > 0 else 0

                gpu_text = f"ğŸ® {gpu_name}  |  ë©”ëª¨ë¦¬: {memory_allocated:.1f}GB / {memory_total_gb:.1f}GB ({memory_usage_percent:.0f}%)"

                # NVIDIA GPU ì‚¬ìš©ë¥ ì€ nvidia-smië¡œ ê°€ì ¸ì˜¤ê¸° (ë³´ì¡°)
                gpu_util_str = ""
                try:
                    result = subprocess.run(
                        ["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"],
                        capture_output=True,
                        text=True,
                        timeout=1
                    )
                    if result.returncode == 0:
                        gpu_util = result.stdout.strip().split('\n')[0].strip()
                        if gpu_util and gpu_util.isdigit():
                            gpu_util_str = f"  |  GPU í™œìš©: {gpu_util}%"
                except Exception as e:
                    logger.debug(f"nvidia-smi error: {str(e)}")

                # AMD GPU ì‚¬ìš©ë¥ ì€ rocm-smië¡œ ê°€ì ¸ì˜¤ê¸° (ë³´ì¡°)
                if not gpu_util_str:
                    try:
                        result = subprocess.run(
                            ["rocm-smi"],
                            capture_output=True,
                            text=True,
                            timeout=1
                        )
                        if result.returncode == 0:
                            logger.debug("AMD ROCm GPU detected")
                    except Exception as e:
                        logger.debug(f"rocm-smi error: {str(e)}")

                # gpu_text += gpu_util_str  # GPU í™œìš©ë¥  í‘œì‹œ (ìˆ¨ê¹€ - ë‚˜ì¤‘ì— í•„ìš”ì‹œ í™œì„±í™”)
            else:
                gpu_text = "ğŸ® GPU not available (CUDA/ROCm disabled)"

        except ImportError:
            logger.debug("PyTorch not available, trying alternative methods...")
            # PyTorch ì—†ìœ¼ë©´ nvidia-smi ë˜ëŠ” rocm-smië¡œ ì‹œë„
            try:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=name,memory.used,memory.total,utilization.gpu", "--format=csv,noheader,nounits"],
                    capture_output=True,
                    text=True,
                    timeout=2
                )
                if result.returncode == 0:
                    output = result.stdout.strip().split('\n')[0]
                    parts = [p.strip() for p in output.split(',')]
                    if len(parts) >= 4:
                        gpu_name = parts[0]
                        memory_used = float(parts[1]) / 1024  # MB to GB
                        memory_total = float(parts[2]) / 1024  # MB to GB
                        gpu_util = parts[3]
                        gpu_text = f"ğŸ® {gpu_name}  |  ë©”ëª¨ë¦¬: {memory_used:.1f}GB / {memory_total:.1f}GB"
                    else:
                        gpu_text = "ğŸ® GPU not detected"
                else:
                    gpu_text = "ğŸ® GPU not detected"
            except Exception as e:
                logger.debug(f"nvidia-smi error: {str(e)}")
                # NVIDIA ì—†ìœ¼ë©´ AMD ROCm ì‹œë„
                try:
                    result = subprocess.run(
                        ["rocm-smi"],
                        capture_output=True,
                        text=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        gpu_text = "ğŸ® AMD ROCm GPU detected"
                    else:
                        gpu_text = "ğŸ® GPU not detected"
                except Exception as e:
                    logger.debug(f"rocm-smi error: {str(e)}")
                    gpu_text = "ğŸ® GPU not detected"

        # ===== CPU ì •ë³´ ì¶”ê°€ (ë¹„ë¸”ë¡œí‚¹ ìƒ˜í”Œë§) =====
        try:
            cpu_percent = psutil.cpu_percent(interval=None)  # Non-blocking (ìµœê·¼ ê°’ ì‚¬ìš©)
            gpu_text += f"  |  CPU: {cpu_percent}%"
        except Exception as e:
            logger.debug(f"CPU info error: {str(e)}")

        return gpu_text

    except Exception as e:
        logger.warning(f"Failed to get display text: {str(e)}")
        return "ğŸ® System info unavailable"
